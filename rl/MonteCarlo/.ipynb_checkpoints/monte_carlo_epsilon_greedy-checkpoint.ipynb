{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Included Control problem- This scripts implement Monte carlo exploring starts method for finding optimal policy.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Included Control problem- This scripts implement Monte carlo exploring starts method for finding optimal policy.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\mohan\\\\Documents\\\\GitHub\\\\Reinforcement-Learning\\\\rl\\\\DynamicProgramming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTION = {'U', 'D', 'L', 'R'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(a, eps=0.1):\n",
    "    '''Choose given \"a\" with probablity 0.5 and some other a'!=a with probablity 0.5/3 '''\n",
    "    p = np.random.random()\n",
    "    if p < (1 - eps):\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice(list(ALL_POSSIBLE_ACTION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(grid, policy):\n",
    "    '''Returns a List of states and corresponding returns\n",
    "    Reset the Game to start at a random position, we need to do this beacuse given our current determinisitc policy, \n",
    "    we would never end up at certain states, but we still want to measure it.(exploring starts method.)'''\n",
    "\n",
    "    s = (2, 0)\n",
    "    grid.set_state(s)\n",
    "    a = random_action(policy.get(s))\n",
    "    \n",
    "    states_action_rewards = [(s, a, 0)] #for a starting state we will give a reward of 0.\n",
    "    stop_itertion = False\n",
    "    #seen_states = set()\n",
    "    #seen_states.add(grid.current_state())\n",
    "    #num_steps = 0\n",
    "    while not stop_itertion:\n",
    "        #num_steps+=1\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        #if s in seen_states:\n",
    "        #    rewards= -2/num_steps\n",
    "        #    states_action_rewards.append((s, None, -100))\n",
    "        #    stop_itertion = True\n",
    "        if grid.game_over():\n",
    "            states_action_rewards.append((s, None, r))\n",
    "            stop_itertion = True\n",
    "        else:\n",
    "            a = policy.get(s)        \n",
    "            a = random_action(a)\n",
    "            states_action_rewards.append((s, a, r))\n",
    "        #seen_states.add(s)\n",
    "\n",
    "    #Calculate the returns by working backwards from the terminal state.\n",
    "    G = 0 #the value of a terminal state is zero by definition\n",
    "    states_actions_returns = []\n",
    "    first = True #True because we are calculating first visit monte carlo\n",
    "\n",
    "    for s, a, r in reversed(states_action_rewards):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "        G = r + (GAMMA * G)\n",
    "    states_actions_returns.reverse()\n",
    "    return states_actions_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "    '''Returns the argmax(key) and max(value) from a dictionary'''\n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "    return max_key, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #using the standard grid again (0 for every step) so that we can compare to iterative policy evaluation.\n",
    "    grid = negative_grid()\n",
    "    \n",
    "    #print rewards\n",
    "    print_values(grid.rewards, grid)\n",
    "    \n",
    "    #Policy - For a given state what is the action we would take.\n",
    "    #set the initial policy to random actions\n",
    "    policy = {}\n",
    "    for s in grid.actions.keys():\n",
    "        policy[s] = np.random.choice(list(ALL_POSSIBLE_ACTION))\n",
    "\n",
    "    #Initialize V(s) and returns \n",
    "    Q = {}\n",
    "    returns = {} #dictionary of a state -> list of returns we have recieved.\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "        if s in grid.actions:\n",
    "            Q[s] = {}\n",
    "            for a in ALL_POSSIBLE_ACTION:\n",
    "                Q[s][a] = 0\n",
    "                returns[(s, a)] = []\n",
    "        else:\n",
    "            pass #terminal state or state we can't otherwise get to.\n",
    "\n",
    "    deltas = []\n",
    "    for t in range(5000):\n",
    "        if t % 100 == 0:\n",
    "            print(t)\n",
    "        \n",
    "        biggest_change = 0\n",
    "        states_actions_returns = play_game(grid, policy)\n",
    "        seen_state_action_pairs = set()\n",
    "        for s, a, G in states_actions_returns:\n",
    "            #We are calculating using first visit monte carlo method. - so we check whether we have seen the state action already \n",
    "            sa = (s, a)\n",
    "            if sa not in seen_state_action_pairs:\n",
    "                old_q = Q[s][a]\n",
    "                returns[sa].append(G)\n",
    "                Q[s][a] = np.mean(returns[sa])\n",
    "                biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "                seen_state_action_pairs.add(sa)\n",
    "        deltas.append(biggest_change)\n",
    "    \n",
    "        #update policy\n",
    "        for s in policy.keys():\n",
    "            policy[s] = max_dict(Q[s])[0]\n",
    "        \n",
    "    plt.plot(deltas)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"final policy: \")\n",
    "    print_policy(policy, grid)\n",
    "    \n",
    "    #find V  \n",
    "    V = {}\n",
    "    for s, Qs in Q.items():\n",
    "        V[s] = max_dict(Q[s])[1]\n",
    "        \n",
    "    print(\"values: \")\n",
    "    print_values(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      " -0.1 -0.1 -0.1  1\n",
      "--------------------\n",
      " -0.1  0 -0.1 -1\n",
      "--------------------\n",
      " -0.1 -0.1 -0.1 -0.1\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final policy: \n",
      "--------------------\n",
      " R | R | R |   |\n",
      "--------------------\n",
      " U |   | U |   |\n",
      "--------------------\n",
      " U | R | U | L |\n",
      "values: \n",
      "--------------------\n",
      "  0.58  0.77  1.0  0\n",
      "--------------------\n",
      "  0.41  0  0.79  0\n",
      "--------------------\n",
      "  0.25  0.38  0.58  0.14\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
